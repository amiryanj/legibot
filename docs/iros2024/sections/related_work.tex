In \cite{dragan2013legibility} Dragan et al. differentiate predictability and legibility, crucial for human-robot collaboration.
They provide formal definitions, propose cost-based models for motion planning, and practically validate the contradiction between predictability and legibility in various characters.

Work \cite{taylor2022observeraware} addresses the problem of the limited field of view of observers.
Their proposed algorithm models observer locations and perspectives, enhancing legibility by placing movements where easily be seen.
The study shows that observer-aware legibility increases the duration of correct goal inferences, but non-targeted observers have lower performance when paths are personalized for others.
The paper emphasizes the importance of considering an observer's environment for effective planning in scenarios like robot-assisted \textbf{restaurant} service.
Also \cite{dragan2015effects} implemented a \textbf{coffee-shop} scenario. Participants collaborated with the robot to fulfill tea orders.
The robot retrieved the appropriate cup, and participants selected ingredients based on the cup being retrieved.

In \cite{bronars2023_GLMM} the core idea is bringing end-to-end framework using conditional generative models to learn legible robot trajectories from multi-modal human demonstrations. 
%
%Zheng et al. \cite{PAT2021IJSR} demonstrated a significant difference in how humans perceive and react to a robot based on its type.
%In their study, when comparing people's reactions to a semi-autonomous wheelchair with a human driver onboard to that of a humanoid robot,
%it became evident that there was a much higher level of respect for the wheelchair robot.

% TODO: to review
* Of course, we all are aware of how signaling lights are used in vehicles to show other people our intentions,
there are also other modalities to communicate the intent can be used,
for example, in IAN by Dugas et al. \cite{dugas2020_IAN} a Pepper robot uses hand gestures and nudges to communicate with the people around it and ask them to clear the way.

* \cite{fukuchi2009focus}

\textbf{Learning-based} approaches have also been used to generate legible motions.
In SLOT-V \cite{XXX} the authors present a method that takes labeled robot trajectories and learns an observer model, using a deep neural network as a value function approximator.
However, assuming that the robot has access to a labeled dataset of legible trajectories is not always realistic.